While recently trying to write down a decoder only transformer (notoriously known with aliases like GPT, LLM, AI and what not), I realised, to start training that model I am using a black box (also notoriously known as TOKENIZER) provided by hugging-face (https://github.com/huggingface/tokenizers). Now until upto this point I knew the tokenizer just has a simple job - to convert text into integers that the transformer can make sense of, and produce sensible output in return.
For my own suprise on further exploring this black box, I realised this is not as simple as I thought. It does much more than returning integers for strings, and does so in an super elegant way.

## An Overview of TOKENIZER
If I had to put it in a simple way and perhaps much more clearer than my previous understandings of it, I'll put it this way:
A Transformer requieres numerical input to perform its magic and develop the sense of language. This numerical input needs to be in such way that we can clearly and distinguishibly map out all the data(ie text/str in case of LLMs).
Now as a general thought as we all get hit with firstly, is that lets just map every unique word in our dataset to a unique integer. This sounds good and does works upto a small scale. An counter argument to this is also simple. If we just look up at any book on our desk, think of training a LLM on that book. Yes, that does seem a lot of unique integers to keep track of. You might have now got a critic against this kind of tokenizer by now. Also to add further more what if a new words pops up during inference (the word that the LLM never saw in training)?
Another idea is to simply map all unique characters in our language/dataset to integers. This sounds much more feasible than previous approach until you come to know about training part of LLMs. If you know about LLMs, the simple downside of character level tokenization is giant sequences which are not feasible for training. If you don't know much about LLMs, then the answer will be the same (I hope you would want to dig more into LLMs, so..).

Now the actual scalable and preferred solution lies btw these two ideas discussed above:
## BPE TOKENIZER
The Byte Pair Tokenization is kind of combination of both word based and character based tokenzation. It settles in middle ground ie sub words, and the BPE algorithm does it so elegantly that it doesn't leave any room for a word that could actually suprise the model at inference time.
